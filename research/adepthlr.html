<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="shortcut icon" href="/favicon.ico"><title>Eric (Ehsan) Qasemi</title><script defer="defer" src="/static/js/main.50ea9f48.js"></script><link href="/static/css/main.ad7beef5.css" rel="stylesheet"><meta charset="utf-8" data-react-helmet="true"><link href="/static/css/responsive.css" rel="stylesheet" type="text/css"></head><body><div id="root"><div><div class="super_container"><header class="header" id="main_header"><div class="header_content d-flex flex-row align-items-center justify-content-start"><div class="logo">Eric (<span>Ehsan</span>) Qasemi</div><div class="main_nav d-flex flex-row align-items-end justify-content-start"><ul class="d-flex flex-row align-items-center justify-content-start"><li class=""><a href="/about">About</a></li><li class=""><a href="/education">Education</a></li><li class=""><a href="/research">Research</a></li><li class=""><a href="/experience">Experience</a></li><li class=""><a href="/skills">Skills</a></li><li class=""><a href="/teaching">Teaching</a></li></ul></div><div class="menu"><div class="menu_content d-flex flex-row align-items-start justify-content-end"><div class="hamburger ml-auto">menu</div><div class="menu_nav text-right"><ul><li><a href="/about">About</a></li><li><a href="/education">Education</a></li><li><a href="/research">Research</a></li><li><a href="/experience">Experience</a></li><li><a href="/skills">Skills</a></li><li><a href="/teaching">Teaching</a></li></ul></div></div></div></div></header><div class="content_container"><div class="main_content_outer d-flex flex-xl-row flex-column align-items-start justify-content-start"><div class="general_info d-flex flex-xl-column flex-md-row flex-column" id="general_information"><div class="general_info_image"><div class="background_image" style="background-image:url(/static/media/me.903a115cfead8380c215.jpg)"></div></div><div class="general_info_content"><div class="general_info_content_inner mCustomScrollbar" data-mcs-theme="minimal-dark"><div class="general_info_title">General Information</div><ul class="general_info_list"><li class="d-flex flex-row align-items-center justify-content-start"><div class="general_info_icon d-flex flex-column align-items-start justify-content-center"><i class="fa fa-envelope" style="color:#8583e1"></i></div><div class="general_info_text"><a href="mailto:qasemi.ehs@gmail.com">qasemi@usc.edu</a></div></li><li class="d-flex flex-row align-items-center justify-content-start"><div class="general_info_icon d-flex flex-column align-items-start justify-content-center"><i class="fa fa-envelope" style="color:#8583e1"></i></div><div class="general_info_text"><a href="mailto:qasemi.ehs@gmail.com">qasemi.ehs@gmail.com</a></div></li><li class="d-flex flex-row align-items-center justify-content-start"><div class="general_info_icon d-flex flex-column align-items-start justify-content-center"><i class="fa fa-home" style="color:#8583e1"></i></div><div class="general_info_text"><a href="http://ehsanqasemi.com">ehsanqasemi.com</a></div></li><li class="d-flex flex-row align-items-center justify-content-start"><div class="general_info_icon d-flex flex-column align-items-start justify-content-center"><i class="fa fa-github" style="color:#8583e1"></i></div><div class="general_info_text"><a href="https://github.com/proska">proska</a></div></li><li class="d-flex flex-row align-items-center justify-content-start"><div class="general_info_icon d-flex flex-column align-items-start justify-content-center"><i class="fa fa-twitter" style="color:#8583e1"></i></div><div class="general_info_text"><a href="https://twitter.com/Proska_">Proska_</a></div></li><li class="d-flex flex-row align-items-center justify-content-start"><div class="general_info_icon d-flex flex-column align-items-start justify-content-center"><i class="fa fa-linkedin" style="color:#8583e1"></i></div><div class="general_info_text"><a href="https://www.linkedin.com/in/ehsan-qasemi-39627477/">ehsan-qasemi</a></div></li></ul></div></div></div><div class="main_content"><div class="main_content_scroll mCustomScrollbar" data-mcs-theme="minimal-dark"><div class="about_content"><div class="research_title">Adversarial Depth Estimation based on Left-Right Consistency</div><div><div class="research_section_title">Highlights:</div><div class="my_research_text"><ul><li style="color:black">-&gt; Depth estimation in urban scenes using Generative-Adversarial Networks (GAN)</li><li style="color:black">-&gt; Full Presentation <a href="https://proska.github.io/ADepthLR/#/overview">here</a></li></ul></div></div><div><div class="research_section_title">Details:</div><div class="my_research_text"><p style="color:black"> <!-- -->Depth estimation or Depth extraction refers to the set of methods and algorithms specifically aimed to obtain an
estimation of the spatial structure of a scene based on the Images or videos of it. In other terms, the goal is to
obtain a measure of the distance of, ideally, each point on these images1
. this notion on predicted depth, is an
essential component in the field of multi-view stereo2 or understanding the 3D geometry of the scene, which is
crucial in many applications3
, e.g. unmanned vehicle, robotics, and human body pose estimation4
.
Historically, researchers have looked at depth estimation problem in supervised learning setup. Models in this
setup attempt to directly predict the depth of each pixel in an image using models that have been trained off-line on
large collections of labeled datasets(ground truth depth data). Although these methods seem simple, there is no
universally acceptable method to generate this type of dataset, for two reasons. First, equipments necessary for
measuring depth data (e.g. laser sensors) are expensive and considered to be extremely noisy which introduced
additional challenge for current machine learning techniques. Second, even a in perfect world with noise-free
affordable equipments, discarding available bank of unlabeled Images and generating large datasets of labeled
data will restrict our model to scenes where labeled data is available. Consequently we need to generate sufficiently
representative and large datasets that requires large time and money investments.
In order to train a model while utilizing the currently available unlabeled (no depth dimension) image datasets,
inspired by humans visual system, researchers have proposed monocular depth estimation methods that exploit
cues such as perspective, lighting, etc. One state-of-art method in this field is based on binocular image generation
using convolutional neural networks4
. In this project, we first aim at recreating the reported results in4
. Additionally
inspired by works in the field of adversarial learning5
, to learn consistency features used in training instead of
hard coding them, we plan to build on the adversarial idea to improve performance and training time of the depth
estimation model.<!-- --> <br></p></div><hr></div><div><div class="research_section_title">Album:</div><br><div><img src="/static/media/adepthLR.d9cffe6fa723c38dc328.jpg" class="img-fluid" alt="Responsive" style="margin-bottom:30px"></div><div><img src="/static/media/adepth_res.935131894d110d0a73cd.png" class="img-fluid" alt="Responsive" style="margin-bottom:30px"></div></div></div></div></div></div></div></div></div></div><div style="text-align:center">Copyright Â©<script>document.write((new Date).getFullYear().toString())</script>2022All rights reserved | This template is made with <i aria-hidden="true" class="fa fa-heart-o"></i> by <a href="https://colorlib.com" target="_blank">Colorlib</a></div></body></html>